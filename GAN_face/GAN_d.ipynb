{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-db0afe6d2c87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    457\u001b[0m                       for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m     \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_compile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_photos_with_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblender_photos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape, InputLayer, Input, Add\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout, LSTM\n",
    "from keras.layers.merge import concatenate as mconc\n",
    "from keras.layers import Concatenate as conc\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.backend import clear_session\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras.layers as layers\n",
    "# from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv2 import imread, imwrite\n",
    "# from imageio import imread, imwrite\n",
    "\n",
    "\n",
    "from LAST_BUILD.gan_settings import _IMG_ROWS, _IMG_COLS, _CHANNEL, \\\n",
    "    _TRAIN_IMG_PATH, _TRUE_PHOTOS_DIR, _BLENDER_PHOTOS_DIR, \\\n",
    "    _ROTATION, _LIGHTNING, \\\n",
    "    _TRAIN_STEPS,  _BATCH_SIZE, _SAVE_INTERVAL, \\\n",
    "    _OUTPUT_IMAGES_X, _OUTPUT_IMAGES_Y, \\\n",
    "    _MOBILENET_INPUT_SHAPE, \\\n",
    "    _GENERATED_FACES_PATH, _INPUT_TENSOR_SHAPE, _PERSONS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "__COLS = 4016\n",
    "__ROWS = 6016\n",
    "\n",
    "def _get_labeled_true_photos(directory):\n",
    "    for path, _, file_names in os.walk(directory):\n",
    "        for file_name in file_names:\n",
    "            for rotation_label in _get_rotation_labels():\n",
    "                for light_label in _get_lightning_labels():\n",
    "\n",
    "                    yield (imread(os.path.join(path, file_name)).reshape(1, _IMG_COLS * _IMG_ROWS * _CHANNEL),\n",
    "                           rotation_label.reshape(1, -1),\n",
    "                           light_label.reshape(1, -1))\n",
    "                    # yield (imread(os.path.join(path, file_name)),\n",
    "                    #        rotation_label,\n",
    "                    #        light_label)\n",
    "\n",
    "def _get_true_photos_paths(directory):\n",
    "    for path, _, file_names in os.walk(directory):\n",
    "        for file_name in file_names:\n",
    "            yield os.path.join(path, file_name)\n",
    "\n",
    "\n",
    "def _get_rotation_labels(rotation=_ROTATION):\n",
    "    return to_categorical(list(range(rotation)))\n",
    "\n",
    "\n",
    "def _get_lightning_labels(light=_LIGHTNING):\n",
    "    return to_categorical(list(range(light)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _open_and_reshape_image(dir=_TRAIN_IMG_PATH,\n",
    "                            rows=_IMG_ROWS,\n",
    "                            cols=_IMG_COLS,\n",
    "                            channel=_CHANNEL):\n",
    "    img_list = ((_TRAIN_IMG_PATH + one_image) for one_image in os.listdir(dir) if one_image.endswith('.jpg'))\n",
    "\n",
    "    x_train = np.array([imread(img_path) for img_path in img_list])\n",
    "\n",
    "    # print(x_train.shape)\n",
    "    # x_train = x_train.reshape(156, channel, rows, cols)\n",
    "    # print(x_train.shape)\n",
    "    return x_train\n",
    "\n",
    "\n",
    "def load_mobilenet_cnn(size_output=None, default_input_shape=_MOBILENET_INPUT_SHAPE, input_tensor_shape=None,\n",
    "                       batch_size=_BATCH_SIZE):\n",
    "    base_model = MobileNet(include_top=False, input_shape=default_input_shape,\n",
    "                           alpha=1, depth_multiplier=1,\n",
    "                           dropout=0.001, weights=\"imagenet\",\n",
    "                           input_tensor=input_tensor_shape, pooling=None)\n",
    "    # if any(input_tensor_shape):\n",
    "    #     base_model.input_tensor = InputLayer(input_shape=input_tensor_shape, batch_size=batch_size)\n",
    "\n",
    "    # add fully connected layers\n",
    "    fc0 = base_model.output\n",
    "    fc0_pool = layers.GlobalAveragePooling2D(data_format='channels_last', name='fc0_pool')(fc0)\n",
    "    fc1 = layers.Dense(256, activation='relu', name='fc1_dense')(fc0_pool)\n",
    "    fc2 = layers.Dense(_IMG_ROWS * _IMG_COLS * _CHANNEL, activation='tanh', name='fc2_dense')(fc1)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=fc2)\n",
    "\n",
    "    # freeze the early layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=_IMG_ROWS, img_cols=_IMG_COLS, channel=_CHANNEL):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "\n",
    "\n",
    "    # (Wâˆ’F+2P)/S+1\n",
    "    def discriminator(self,\n",
    "                      input_output_return=False):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        # self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        self.x1 = Flatten()(inputs)\n",
    "        self.x2 = Dense(32, activation='relu')(self.x1)\n",
    "        self.x3 = Dense(16, activation='relu')(self.x2)\n",
    "        self.x4 = Dense(8, activation='sigmoid')(self.x3)\n",
    "        self.pred = Dense(1, activation='softmax')(self.x4)\n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=self.pred)\n",
    "\n",
    "        if input_output_return:\n",
    "            return inputs, self.pred\n",
    "        else:\n",
    "            return self.model\n",
    "\n",
    "\n",
    "\n",
    "    def generator(self,\n",
    "                   _compile=False,\n",
    "                   input_output_return=False):\n",
    "\n",
    "        if self.G:\n",
    "            return self.G\n",
    "\n",
    "        dropout = 0.8\n",
    "\n",
    "        reshape_param_1 = 150\n",
    "        reshape_param_2 = 3\n",
    "        self.depth = reshape_param_1 * reshape_param_1 * reshape_param_2\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.input_image = Input(shape=(_IMG_COLS * _IMG_ROWS * _CHANNEL,), dtype='float32')\n",
    "        self.input_image_dense = Dense(64, activation='relu')(self.input_image)\n",
    "\n",
    "        self.input_label_rotation = Input(shape=(_ROTATION,), dtype='float32')\n",
    "        self.input_label_rotation_dense = Dense(64, activation='relu')(self.input_label_rotation)\n",
    "\n",
    "        self.input_label_lightning = Input(shape=(_LIGHTNING,), dtype='float32')\n",
    "        self.input_label_lightning_dense = Dense(64, activation='relu')(self.input_label_lightning)\n",
    "\n",
    "        generator_inputs = [self.input_image, self.input_label_rotation, self.input_label_lightning]\n",
    "        self.generator_input_denses = [self.input_image_dense, self.input_label_rotation_dense, self.input_label_lightning_dense]\n",
    "\n",
    "        self.merged_inputs = Add()(self.generator_input_denses)\n",
    "\n",
    "        self.x1 = Dense(self.depth, activation='relu')(self.merged_inputs)\n",
    "        # x1 = Dense(depth), activation='relu')(x1)\n",
    "        self.x1_reshaped = Reshape((reshape_param_1, reshape_param_1, reshape_param_2))(self.x1)\n",
    "        self.x1_out = Dropout(dropout)(self.x1_reshaped)\n",
    "\n",
    "        self.x2 = UpSampling2D()(self.x1_out)\n",
    "        self.x2_conv2d = Conv2DTranspose(int(self.depth / 200), 5, padding='same')(self.x2)\n",
    "        self.x2_batch_norm = BatchNormalization(momentum=0.9)(self.x2_conv2d)\n",
    "        self.x2_out = Activation(activation='relu')(self.x2_batch_norm)\n",
    "\n",
    "        self.x3 = UpSampling2D()(self.x2_out)\n",
    "        self.x3_conv2d = Conv2DTranspose(int(self.depth / 400), 5, padding='same')(self.x3)\n",
    "        self.x3_batch_norm = BatchNormalization(momentum=0.9)(self.x3_conv2d)\n",
    "        self.x3_out = Activation(activation='relu')(self.x3_batch_norm)\n",
    "\n",
    "        self.x4_conv2d = Conv2DTranspose(int(self.depth / 800), 5, padding='same')(self.x3_out)\n",
    "        self.x4_batch_norm = BatchNormalization(momentum=0.9)(self.x4_conv2d)\n",
    "        self.x4_out = Activation(activation='relu')(self.x4_batch_norm)\n",
    "\n",
    "        self.x5_conv2d = Conv2DTranspose(1, 5, padding='same')(self.x4_out)\n",
    "        self.x5_out = Activation(activation='relu')(self.x5_conv2d)\n",
    "\n",
    "        # self.X6 = Dense(int(depth/16), activation='relu')(self.x5_out)\n",
    "        self.x6 = Dense((3), activation='relu')(self.x5_out)\n",
    "\n",
    "        # x6_a = Activation(activation='relu')\n",
    "\n",
    "        model = Model(inputs=[self.input_image, self.input_label_rotation, self.input_label_lightning], outputs=self.x6)\n",
    "        opt = RMSprop(lr=0.0001, decay=3e-8)\n",
    "\n",
    "        if _compile:\n",
    "            model.compile(loss='mean_squard_error', optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "\n",
    "        if input_output_return:\n",
    "            return generator_inputs, self.x6\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                        metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        # self.AM = Sequential()\n",
    "        # self.AM.add(self.generator())\n",
    "        # self.AM.add(self.discriminator())\n",
    "        #\n",
    "        #\n",
    "        # # self.AM.add(self.generator())\n",
    "        # # self.AM.add(self.discriminator_model())\n",
    "        #\n",
    "        # self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "        #                 metrics=['accuracy'])\n",
    "        # return self.AM\n",
    "\n",
    "        # am = Add()(inputs=(self.generator(),self.discriminator()))\n",
    "\n",
    "\n",
    "        #\n",
    "        # am_model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "        #                 metrics=['accuracy'])\n",
    "        \"\"\"        print(type(self.discriminator()(inputs)))\n",
    "        merge = conc()([self.generator().outputs,self.discriminator().inputs])\n",
    "\n",
    "        hidden1 = Dense(10, activation='relu')(merge)\n",
    "        output = Dense(1, activation='sigmoid')(hidden1)\n",
    "\n",
    "        self.AM = Model(inputs=self.generator().inputs, outputs=output)\n",
    "\n",
    "        self.AM.summary()\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # gen_inputs, gen_outputs = self.generator(input_output_return=True)\n",
    "        # dis_inputs, dis_outputs = self.discriminator(input_output_return=True)\n",
    "        #\n",
    "        # adv_input_image_dense = Dense(512, activation='relu')(gen_inputs[0])\n",
    "        # adv_input_label_rotation_dense = Dense(512, activation='relu')(gen_inputs[1])\n",
    "        # adv_input_label_lightning_dense = Dense(512, activation='relu')(gen_inputs[2])\n",
    "        #\n",
    "        # adv_merged_inputs = Add()([adv_input_image_dense, adv_input_label_rotation_dense, adv_input_label_lightning_dense])\n",
    "\n",
    "        gen = self.generator()\n",
    "        dis = self.discriminator()(gen.output)\n",
    "        # print(dis.input)\n",
    "        # print(type(dis.input))\n",
    "        #\n",
    "        # dis.input = gen.output\n",
    "        #\n",
    "        # print(dis.input)\n",
    "        # print(type(dis.input))\n",
    "\n",
    "\n",
    "\n",
    "        # adv_top_input = gen.generator_input_denses\n",
    "        #\n",
    "        # adv_top_merged_input = Add()(adv_top_input)\n",
    "        #\n",
    "        # x1 = Dense(gen.depth, activation='relu')(adv_top_merged_input)\n",
    "        #\n",
    "        # adv_medium_merged_input = Add()([x1,dis.inp])\n",
    "        #\n",
    "        # # x2 = Dense(gen.depth, activation='relu')(adv_medium_merged_input)\n",
    "        #\n",
    "        # self.AM = Model(inputs=adv_top_input, outputs=dis.outputs)\n",
    "\n",
    "\n",
    "        self.AM = Model(inputs=gen.inputs, outputs=dis)\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = _IMG_ROWS\n",
    "        self.img_cols = _IMG_COLS\n",
    "        self.channel = _CHANNEL\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\",\n",
    "                                                 one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,\n",
    "                                            self.img_cols, 1).astype(np.float32)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.x_train = _open_and_reshape_image()\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator = self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "\n",
    "    def train(self, train_steps=_TRAIN_STEPS, batch_size=_BATCH_SIZE, save_interval=_SAVE_INTERVAL,\n",
    "              true_photos=_TRUE_PHOTOS_DIR):\n",
    "\n",
    "        true_photos_with_labels = _get_labeled_true_photos(true_photos)\n",
    "\n",
    "        blender_photos = (imread(blender_photo) for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))\n",
    "\n",
    "\n",
    "        for i, true_photo_with_labels, blender_photo in zip(range(_ROTATION * _LIGHTNING * _PERSONS),\n",
    "                                                            true_photos_with_labels, blender_photos):\n",
    "            blender_photo = np.expand_dims(blender_photo, axis=0)\n",
    "            img, rot_lbl, light_lbl = true_photo_with_labels\n",
    "            rot_lbl = rot_lbl.reshape(1, -1)\n",
    "            light_lbl = light_lbl.reshape(1, -1)\n",
    "\n",
    "            images_fake = self.generator.predict([img, rot_lbl, light_lbl])\n",
    "            imwrite('face_{}.jpg'.format(i), images_fake[0])\n",
    "\n",
    "            # print(blender_photo.shape, images_fake.shape)\n",
    "            x = np.concatenate((blender_photo, images_fake))\n",
    "            y = np.ones([2 * batch_size, 1])\n",
    "            # print(x.shape,y.shape)\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "\n",
    "            a_loss = self.adversarial.train_on_batch([img, rot_lbl, light_lbl], y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval > 0:\n",
    "                if (i + 1) % save_interval == 0:\n",
    "                    self.plot_images(save2file=True, samples=9,\n",
    "                                     input=[img, rot_lbl, light_lbl], step=(i + 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=9, input=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if input is None:\n",
    "                # noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "\n",
    "                input = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = _GENERATED_FACES_PATH\n",
    "                filename += \"face_{}.png\".format(str(100001 + step))\n",
    "            images = self.generator.predict(input)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            # print(i)\n",
    "            # print(images.shape)\n",
    "            plt.subplot(_OUTPUT_IMAGES_X, _OUTPUT_IMAGES_Y, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols, self.channel])\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            # print(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    true_photos_with_labels = _get_labeled_true_photos(_TRUE_PHOTOS_DIR)\n",
    "\n",
    "    blender_photos = (np.expand_dims(imread(blender_photo), axis=0) \n",
    "                      for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))\n",
    "    \n",
    "    gen = generator(_compile=True)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test_split(list(true_photos_with_labels), list(blender_photos), test_size=0.2)\n",
    "    \n",
    "    gen.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=2)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     def train(gen, train_steps=_TRAIN_STEPS, batch_size=_BATCH_SIZE, save_interval=_SAVE_INTERVAL,\n",
    "#               true_photos=_TRUE_PHOTOS_DIR):\n",
    "        \n",
    "        \n",
    "\n",
    "#         for i, true_photo_with_labels, blender_photo in zip(range(_ROTATION * _LIGHTNING * _PERSONS),\n",
    "#                                                             true_photos_with_labels, blender_photos):\n",
    "#             blender_photo = np.expand_dims(blender_photo, axis=0)\n",
    "#         img, rot_lbl, light_lbl = true_photo_with_labels\n",
    "\n",
    "#         images_fake = gen.predict([img, rot_lbl, light_lbl])\n",
    "#         imwrite('face_{}.jpg'.format(i), images_fake[0])\n",
    "#         gen.train_on_batch([img, rot_lbl, light_lbl], blender_photo)\n",
    "#         gen.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "    \n",
    "#     train(gen)\n",
    "    \n",
    "#     mnist_dcgan = MNIST_DCGAN()\n",
    "#     timer = ElapsedTimer()\n",
    "#     # mnist_dcgan.adversarial.layers()\n",
    "#     # mnist_dcgan.train(train_steps=50000, batch_size=_BATCH_SIZE, save_interval=10)\n",
    "#     mnist_dcgan.train(train_steps=50000, batch_size=_BATCH_SIZE, save_interval=3)\n",
    "#     timer.elapsed_time()\n",
    "#     mnist_dcgan.plot_images(fake=True)\n",
    "#     # mnist_dcgan.plot_images(fake=False, save2file=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen, train_steps=_TRAIN_STEPS, batch_size=_BATCH_SIZE, save_interval=_SAVE_INTERVAL,\n",
    "              true_photos=_TRUE_PHOTOS_DIR):\n",
    "    true_photos_with_labels = _get_labeled_true_photos(_TRUE_PHOTOS_DIR)\n",
    "\n",
    "    blender_photos = (imread(blender_photo) for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))\n",
    "\n",
    "\n",
    "    for i, true_photo_with_labels, blender_photo in zip(range(_ROTATION * _LIGHTNING * _PERSONS),\n",
    "                                                        true_photos_with_labels, blender_photos):\n",
    "        blender_photo = np.expand_dims(blender_photo, axis=0)\n",
    "        img, rot_lbl, light_lbl = true_photo_with_labels\n",
    "        rot_lbl = rot_lbl.reshape(1, -1)\n",
    "        light_lbl = light_lbl.reshape(1, -1)\n",
    "\n",
    "        images_fake = gen.predict([img, rot_lbl, light_lbl])\n",
    "        imwrite('face_{}.jpg'.format(i), images_fake[0])\n",
    "        gen.fit([img, rot_lbl, light_lbl], blender_photo, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(_compile=False,\n",
    "              input_output_return=False):\n",
    "        dropout = 0.8\n",
    "\n",
    "        reshape_param_1 = 150\n",
    "        reshape_param_2 = 3\n",
    "        depth = reshape_param_1 * reshape_param_1 * reshape_param_2\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        input_image = Input(shape=(_IMG_COLS * _IMG_ROWS * _CHANNEL,), dtype='float32')\n",
    "        input_image_dense = Dense(64, activation='relu')(input_image)\n",
    "\n",
    "        input_label_rotation = Input(shape=(_ROTATION,), dtype='float32')\n",
    "        input_label_rotation_dense = Dense(64, activation='relu')(input_label_rotation)\n",
    "\n",
    "        input_label_lightning = Input(shape=(_LIGHTNING,), dtype='float32')\n",
    "        input_label_lightning_dense = Dense(64, activation='relu')(input_label_lightning)\n",
    "\n",
    "        generator_inputs = [input_image, input_label_rotation, input_label_lightning]\n",
    "        generator_input_denses = [input_image_dense, input_label_rotation_dense, input_label_lightning_dense]\n",
    "\n",
    "        merged_inputs = Add()(generator_input_denses)\n",
    "\n",
    "        x1 = Dense(depth, activation='relu')(merged_inputs)\n",
    "        # x1 = Dense(depth), activation='relu')(x1)\n",
    "        x1_reshaped = Reshape((reshape_param_1, reshape_param_1, reshape_param_2))(x1)\n",
    "        x1_out = Dropout(dropout)(x1_reshaped)\n",
    "\n",
    "        x2 = UpSampling2D()(x1_out)\n",
    "        x2_conv2d = Conv2DTranspose(int(depth / 200), 5, padding='same')(x2)\n",
    "        x2_batch_norm = BatchNormalization(momentum=0.9)(x2_conv2d)\n",
    "        x2_out = Activation(activation='relu')(x2_batch_norm)\n",
    "\n",
    "        x3 = UpSampling2D()(x2_out)\n",
    "        x3_conv2d = Conv2DTranspose(int(depth / 400), 5, padding='same')(x3)\n",
    "        x3_batch_norm = BatchNormalization(momentum=0.9)(x3_conv2d)\n",
    "        x3_out = Activation(activation='relu')(x3_batch_norm)\n",
    "\n",
    "        x4_conv2d = Conv2DTranspose(int(depth / 800), 5, padding='same')(x3_out)\n",
    "        x4_batch_norm = BatchNormalization(momentum=0.9)(x4_conv2d)\n",
    "        x4_out = Activation(activation='relu')(x4_batch_norm)\n",
    "\n",
    "        x5_conv2d = Conv2DTranspose(1, 5, padding='same')(x4_out)\n",
    "        x5_out = Activation(activation='relu')(x5_conv2d)\n",
    "\n",
    "        # self.X6 = Dense(int(depth/16), activation='relu')(self.x5_out)\n",
    "        x6 = Dense((3), activation='relu')(x5_out)\n",
    "\n",
    "        # x6_a = Activation(activation='relu')\n",
    "\n",
    "        model = Model(inputs=[input_image, input_label_rotation, input_label_lightning], outputs=x6)\n",
    "        opt = RMSprop(lr=0.001, decay=0.1, epsilon=0.1)\n",
    "\n",
    "        if _compile:\n",
    "            model.compile(loss='mean_squared_error', optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "        from contextlib import redirect_stdout\n",
    "\n",
    "        with open('modelsummary.txt', 'w') as f:\n",
    "            with redirect_stdout(f):\n",
    "                model.summary()\n",
    "\n",
    "\n",
    "\n",
    "        if input_output_return:\n",
    "            return generator_inputs, x6\n",
    "        else:\n",
    "            return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1080000)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           69120064    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           7744        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 67500)        4387500     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 150, 150, 3)  0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 150, 3)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 300, 300, 3)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 300, 300, 337 25612       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 300, 300, 337 1348        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300, 300, 337 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 600, 600, 337 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 600, 600, 168 1415568     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 600, 600, 168 672         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 600, 600, 168 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 600, 600, 84) 352884      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 600, 600, 84) 336         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 600, 600, 84) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 600, 600, 1)  2101        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 600, 600, 1)  0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 600, 600, 3)  6           activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 75,314,731\n",
      "Trainable params: 75,313,553\n",
      "Non-trainable params: 1,178\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = generator(_compile=True)\n",
    "# train(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_photos = (np.expand_dims(imread(blender_photo), axis=0) \n",
    "                      for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1080000)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           69120064    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           7744        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 67500)        4387500     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 150, 150, 3)  0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 150, 3)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 300, 300, 3)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 300, 300, 337 25612       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 300, 300, 337 1348        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300, 300, 337 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 600, 600, 337 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 600, 600, 168 1415568     up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 600, 600, 168 672         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 600, 600, 168 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 600, 600, 84) 352884      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 600, 600, 84) 336         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 600, 600, 84) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 600, 600, 1)  2101        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 600, 600, 1)  0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 600, 600, 3)  6           activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 75,314,731\n",
      "Trainable params: 75,313,553\n",
      "Non-trainable params: 1,178\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "true_photos_with_labels = _get_labeled_true_photos(_TRUE_PHOTOS_DIR)\n",
    "\n",
    "blender_photos = (np.expand_dims(imread(blender_photo), axis=0) \n",
    "                  for blender_photo in _get_true_photos_paths(_TRAIN_IMG_PATH))\n",
    "\n",
    "gen = generator(_compile=True)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(list(true_photos_with_labels), list(blender_photos), test_size=0.2)\n",
    "\n",
    "gen.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
